<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ë©πÈîü | Kun Zhan </title> <meta name="author" content="Kun Zhan"> <meta name="description" content="Kun Zhan's personal website. Autonomous driving expert, AI researcher, and cognitive intelligence lead at Li Auto. "> <meta name="keywords" content="autonomous driving, computer vision, 3D vision, large language models, world models, AI, machine learning, robotics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%97&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhankunliauto.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Ë©πÈîü | Kun Zhan </h1> <p class="desc">Cognitive Intelligence Lead at <a href="https://www.lixiang.com/" rel="external nofollow noopener" target="_blank">Li Auto</a> | Autonomous Driving Expert | AI Researcher</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portrait-480.webp 480w,/assets/img/portrait-800.webp 800w,/assets/img/portrait-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/portrait.jpeg?6431d2f88790737681962b94668560ba" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="portrait.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Email: zk_1028@aliyun.com</p> <p>WeChat: KevinZhan1990</p> <p>Beijing, China</p> </div> </div> <div class="clearfix"> <p><a href="https://scholar.google.com/citations?user=1J061HIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Google%20Scholar-4285F4?style=flat&amp;logo=google-scholar&amp;logoColor=white" alt="Google Scholar"></a> <a href="mailto:zk_1028@aliyun.com"><img src="https://img.shields.io/badge/Email-zk__1028%40aliyun.com-D14836?style=flat&amp;logo=gmail&amp;logoColor=white" alt="Email"></a> <a href=""><img src="https://img.shields.io/badge/Phone-15210600944-25D366?style=flat&amp;logo=whatsapp&amp;logoColor=white" alt="Phone"></a></p> <h2 id="-about-me">üî≠ About Me</h2> <p>Hello, I‚Äôm Kun Zhan. I currently serve as the Cognitive Intelligence Lead at Li Auto, where I lead a team focused on the research and implementation of cognitive models, world models, and reinforcement learning.</p> <p>My research ‚Äúambitions‚Äù are vast: autonomous driving, computer vision, 3D vision, large language models, embodied intelligence‚Ä¶ Essentially, I‚Äôm passionate about any technology that can make vehicles ‚Äúsmarter‚Äù! I‚Äôm particularly fascinated by implementing cutting-edge AI technologies into robots and vehicles, turning science fiction scenarios into reality and working toward an autonomous future.</p> <p>For me, technological innovation isn‚Äôt just about theoretical breakthroughs‚Äîit‚Äôs about practical applications that can genuinely transform how people travel. I hope to contribute to the advancement of autonomous driving technology through continuous exploration.</p> <h2 id="-research-interests">üåü Research Interests</h2> <ul> <li> <strong>Autonomous Driving</strong>: End-to-end autonomous driving systems, decision-making and planning</li> <li> <strong>Computer Vision</strong>: Object detection and tracking, scene understanding</li> <li> <strong>3D Vision</strong>: 3D perception, reconstruction, and modeling</li> <li> <strong>Large Language Models</strong>: Applications of multimodal large models in autonomous driving</li> <li> <strong>World Models</strong>: Environment modeling and prediction, reinforcement learning</li> </ul> <h2 id="-work-experience">üíº Work Experience</h2> <h3 id="li-auto--april-2021---present">Li Auto | April 2021 - Present</h3> <p>Cognitive Intelligence Lead, directing a team in developing cutting-edge autonomous driving technologies, with a focus on cognitive models, world models, and reinforcement learning research and implementation.</p> <h3 id="baidu--april-2016---march-2021">Baidu | April 2016 - March 2021</h3> <p>Autonomous Driving Researcher, involved in developing computer vision and artificial intelligence solutions for autonomous vehicles.</p> <h2 id="-academic-achievements">üìö Academic Achievements</h2> <h3 id="citation-statistics">Citation Statistics</h3> <ul> <li>Total Citations: 286+</li> <li>h-index: 7</li> <li>i10-index: 6</li> </ul> <h3 id="selected-publications">Selected Publications</h3> <ol> <li> <p><strong>Drivevlm: The convergence of autonomous driving and large vision-language models</strong> (2024) X Tian, J Gu, B Li, Y Liu, Y Wang, Z Zhao, <strong>K Zhan</strong>, P Jia, X Lang, H Zhao <em>arXiv preprint arXiv:2402.12289</em> | Citations: 107</p> </li> <li> <p><strong>Street gaussians: Modeling dynamic urban scenes with gaussian splatting</strong> (2024) Y Yan, H Lin, C Zhou, W Wang, H Sun, <strong>K Zhan</strong>, X Lang, X Zhou, S Peng <em>European Conference on Computer Vision, 156-173</em> | Citations: 102</p> </li> <li> <p><strong>Planagent: A multi-modal large language agent for closed-loop vehicle motion planning</strong> (2024) Y Zheng, Z Xing, Q Zhang, B Jin, P Li, Y Zheng, Z Xia, <strong>K Zhan</strong>, X Lang, D Zhao <em>arXiv preprint arXiv:2406.01587</em> | Citations: 12</p> </li> <li> <p><strong>Tod3cap: Towards 3d dense captioning in outdoor scenes</strong> (2024) B Jin, Y Zheng, P Li, W Li, Y Zheng, S Hu, X Liu, J Zhu, Z Yan, H Sun, <strong>K Zhan</strong>, X Lang, P Jia <em>European Conference on Computer Vision, 367-384</em> | Citations: 10</p> </li> <li> <p><strong>Unleashing generalization of end-to-end autonomous driving with controllable long video generation</strong> (2024) E Ma, L Zhou, T Tang, Z Zhang, D Han, J Jiang, <strong>K Zhan</strong>, P Jia, X Lang, K Yu <em>arXiv preprint arXiv:2406.01349</em> | Citations: 9</p> </li> </ol> <h3 id="patents">Patents</h3> <ul> <li>Eight U.S. and Chinese patents pending</li> </ul> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 05, 2024</th> <td> New preprint: ‚ÄúPlanAgent: A Multi-modal Large Language Agent for Closed-loop Vehicle Motion Planning‚Äù is now available on arXiv. This work introduces a novel approach for autonomous vehicle planning using large language models. </td> </tr> <tr> <th scope="row" style="width: 20%">May 15, 2024</th> <td> <a class="news-title" href="/news/announcement_2/">Street Gaussians accepted to ECCV 2024</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 10, 2024</th> <td> Our paper ‚ÄúTOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes‚Äù has been accepted to ECCV 2024! This work enhances scene understanding for autonomous systems through detailed 3D captioning. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 20, 2024</th> <td> Our paper ‚ÄúDriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models‚Äù is now available on arXiv! This work integrates LVLMs with autonomous driving systems for more intuitive human-vehicle interaction. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://journals.aps.org/" rel="external nofollow noopener" target="_blank">PhysRev</a> </abbr> </div> <div id="PhysRev.47.777" class="col-sm-8"> <div class="title">Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?</div> <div class="author"> <em>A. Einstein<sup>*‚Ä†</sup></em>,¬†<a href="https://en.wikipedia.org/wiki/Boris_Podolsky" rel="external nofollow noopener" target="_blank">B. Podolsky<sup>*</sup></a>,¬†and¬†<a href="https://en.wikipedia.org/wiki/Nathan_Rosen" rel="external nofollow noopener" target="_blank">N. Rosen<sup>*</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Example use of superscripts&lt;br&gt;‚Ä† Albert Einstein"> </i> </div> <div class="periodical"> <em>Phys. Rev.</em>, New Jersey. <em>More Information</em> can be <a href="https://github.com/alshedivat/al-folio/" rel="external nofollow noopener" target="_blank">found here</a> , May 1935 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1103/PhysRev.47.777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-altmetric-id="248277"></span> <span class="__dimensions_badge_embed__" data-doi="10.1103/PhysRev.47.777" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=1J061HIAAAAJ&amp;citation_for_view=1J061HIAAAAJ:qyhmnyLat1gC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> <a href="https://inspirehep.net/literature/3255" aria-label="Inspirehep link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/inspire-4.2K-001628?logo=inspire&amp;logoColor=001628&amp;labelColor=beige" alt="4.2K InspireHEP citations"> </a> </div> <div class="abstract hidden"> <p>In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/drivevlm-480.webp 480w,/assets/img/publication_preview/drivevlm-800.webp 800w,/assets/img/publication_preview/drivevlm-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/drivevlm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="drivevlm.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tian2024drivevlm" class="col-sm-8"> <div class="title">Drivevlm: The convergence of autonomous driving and large vision-language models</div> <div class="author"> Xiaoyu Tian,¬†Jianfeng Gu,¬†Bingqian Li, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Yiyang Liu, Yuhang Wang, Zhenyu Zhao, Kun Zhan, Ping Jia, Xiaoxue Lang, Hang Zhao' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.12289</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2402.12289" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>This paper introduces DriveVLM, a novel approach that integrates large vision-language models (LVLMs) with autonomous driving systems. DriveVLM enables natural language interaction with autonomous vehicles, allowing for intuitive communication about driving scenarios, decision-making processes, and environmental perception. The system combines the reasoning capabilities of LVLMs with specialized driving knowledge, creating a more transparent and user-friendly autonomous driving experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/street_gaussians-480.webp 480w,/assets/img/publication_preview/street_gaussians-800.webp 800w,/assets/img/publication_preview/street_gaussians-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/street_gaussians.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="street_gaussians.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yan2024street" class="col-sm-8"> <div class="title">Street gaussians: Modeling dynamic urban scenes with gaussian splatting</div> <div class="author"> Yuanxing Yan,¬†Hao Lin,¬†Cheng Zhou, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Wenshan Wang, Hang Sun, Kun Zhan, Xiaoxue Lang, Xiwu Zhou, Shuaicheng Peng' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2401.01339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Street Gaussians presents a novel approach for modeling dynamic urban scenes using 3D Gaussian Splatting. This method effectively captures both static and dynamic elements in complex urban environments, providing high-quality, real-time rendering capabilities. The approach addresses challenges in representing moving objects, varying lighting conditions, and complex geometries, making it particularly valuable for autonomous driving simulation and training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/planagent-480.webp 480w,/assets/img/publication_preview/planagent-800.webp 800w,/assets/img/publication_preview/planagent-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/planagent.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="planagent.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zheng2024planagent" class="col-sm-8"> <div class="title">Planagent: A multi-modal large language agent for closed-loop vehicle motion planning</div> <div class="author"> Yiran Zheng,¬†Zhengyuan Xing,¬†Qingwen Zhang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Boyi Jin, Pengfei Li, Yifan Zheng, Zhengyu Xia, Kun Zhan, Xiaoxue Lang, Dongbin Zhao' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.01587</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.01587" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>PlanAgent introduces a multi-modal large language agent framework for closed-loop vehicle motion planning. This innovative approach leverages large language models to interpret complex driving scenarios, reason about traffic rules and safety constraints, and generate appropriate motion plans. The agent integrates visual perception, natural language reasoning, and specialized driving knowledge to create a more interpretable and adaptable planning system for autonomous vehicles.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tod3cap-480.webp 480w,/assets/img/publication_preview/tod3cap-800.webp 800w,/assets/img/publication_preview/tod3cap-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/tod3cap.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tod3cap.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="jin2024tod3cap" class="col-sm-8"> <div class="title">Tod3cap: Towards 3d dense captioning in outdoor scenes</div> <div class="author"> Boyi Jin,¬†Yiran Zheng,¬†Pengfei Li, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Wenhai Li, Yifan Zheng, Shuai Hu, Xingyu Liu, Jingdong Zhu, Zhengyu Yan, Hang Sun, Kun Zhan, Xiaoxue Lang, Ping Jia' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2403.09427" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>TOD3Cap presents a novel approach for 3D dense captioning in outdoor scenes, particularly focusing on autonomous driving environments. This method generates detailed textual descriptions of objects and regions within 3D space, enhancing scene understanding for autonomous systems. The approach combines advanced 3D perception with natural language generation, creating a more comprehensive representation of the driving environment that can be used for improved decision-making and human-vehicle interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/video_generation-480.webp 480w,/assets/img/publication_preview/video_generation-800.webp 800w,/assets/img/publication_preview/video_generation-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/video_generation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="video_generation.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ma2024unleashing" class="col-sm-8"> <div class="title">Unleashing generalization of end-to-end autonomous driving with controllable long video generation</div> <div class="author"> Erkang Ma,¬†Lina Zhou,¬†Tianyu Tang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Zhiyu Zhang, Dong Han, Jiangtao Jiang, Kun Zhan, Ping Jia, Xiaoxue Lang, Kai Yu' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.01349</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.01349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>This research introduces a novel approach for enhancing end-to-end autonomous driving systems through controllable long video generation. The method addresses the generalization challenges in autonomous driving by generating diverse, realistic driving scenarios for training and evaluation. By leveraging advanced video generation techniques with specific control mechanisms, the system can create extended driving sequences that maintain temporal consistency while representing a wide range of driving conditions and edge cases.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A%6B_%31%30%32%38@%61%6C%69%79%75%6E.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=1J061HIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://wa.me/15210600944" title="whatsapp" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-whatsapp"></i></a> <a href="https://www.alberteinstein.com/" title="Custom Social" rel="external nofollow noopener" target="_blank"> <img src="https://www.alberteinstein.com/wp-content/uploads/2024/03/cropped-favicon-192x192.png" alt="Custom Social"> </a> </div> <div class="contact-note">Feel free to reach out via email or phone for collaborations, research discussions, or opportunities. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Kun Zhan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>