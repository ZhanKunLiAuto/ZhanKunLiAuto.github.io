---
---

@string{aps = {American Physical Society,}}

@article{tian2024drivevlm,
  abbr={arXiv},
  title={Drivevlm: The convergence of autonomous driving and large vision-language models},
  author={Tian, Xiaoyu and Gu, Jianfeng and Li, Bingqian and Liu, Yiyang and Wang, Yuhang and Zhao, Zhenyu and Zhan, Kun and Jia, Ping and Lang, Xiaoxue and Zhao, Hang},
  journal={arXiv preprint arXiv:2402.12289},
  year={2024},
  selected={true},
  preview={drivevlm.png},
  abstract={This paper introduces DriveVLM, a novel approach that integrates large vision-language models (LVLMs) with autonomous driving systems. DriveVLM enables natural language interaction with autonomous vehicles, allowing for intuitive communication about driving scenarios, decision-making processes, and environmental perception. The system combines the reasoning capabilities of LVLMs with specialized driving knowledge, creating a more transparent and user-friendly autonomous driving experience.},
  html={https://arxiv.org/abs/2402.12289}
}

@inproceedings{yan2024street,
  abbr={ECCV},
  title={Street gaussians: Modeling dynamic urban scenes with gaussian splatting},
  author={Yan, Yuanxing and Lin, Hao and Zhou, Cheng and Wang, Wenshan and Sun, Hang and Zhan, Kun and Lang, Xiaoxue and Zhou, Xiwu and Peng, Shuaicheng},
  booktitle={European Conference on Computer Vision},
  pages={156--173},
  year={2024},
  organization={Springer},
  selected={true},
  preview={street_gaussians.png},
  abstract={Street Gaussians presents a novel approach for modeling dynamic urban scenes using 3D Gaussian Splatting. This method effectively captures both static and dynamic elements in complex urban environments, providing high-quality, real-time rendering capabilities. The approach addresses challenges in representing moving objects, varying lighting conditions, and complex geometries, making it particularly valuable for autonomous driving simulation and training.},
  html={https://arxiv.org/abs/2401.01339}
}

@article{zheng2024planagent,
  abbr={arXiv},
  title={Planagent: A multi-modal large language agent for closed-loop vehicle motion planning},
  author={Zheng, Yiran and Xing, Zhengyuan and Zhang, Qingwen and Jin, Boyi and Li, Pengfei and Zheng, Yifan and Xia, Zhengyu and Zhan, Kun and Lang, Xiaoxue and Zhao, Dongbin},
  journal={arXiv preprint arXiv:2406.01587},
  year={2024},
  selected={true},
  preview={planagent.png},
  abstract={PlanAgent introduces a multi-modal large language agent framework for closed-loop vehicle motion planning. This innovative approach leverages large language models to interpret complex driving scenarios, reason about traffic rules and safety constraints, and generate appropriate motion plans. The agent integrates visual perception, natural language reasoning, and specialized driving knowledge to create a more interpretable and adaptable planning system for autonomous vehicles.},
  html={https://arxiv.org/abs/2406.01587}
}

@inproceedings{jin2024tod3cap,
  abbr={ECCV},
  title={Tod3cap: Towards 3d dense captioning in outdoor scenes},
  author={Jin, Boyi and Zheng, Yiran and Li, Pengfei and Li, Wenhai and Zheng, Yifan and Hu, Shuai and Liu, Xingyu and Zhu, Jingdong and Yan, Zhengyu and Sun, Hang and Zhan, Kun and Lang, Xiaoxue and Jia, Ping},
  booktitle={European Conference on Computer Vision},
  pages={367--384},
  year={2024},
  organization={Springer},
  selected={true},
  preview={tod3cap.png},
  abstract={TOD3Cap presents a novel approach for 3D dense captioning in outdoor scenes, particularly focusing on autonomous driving environments. This method generates detailed textual descriptions of objects and regions within 3D space, enhancing scene understanding for autonomous systems. The approach combines advanced 3D perception with natural language generation, creating a more comprehensive representation of the driving environment that can be used for improved decision-making and human-vehicle interaction.},
  html={https://arxiv.org/abs/2403.09427}
}

@article{ma2024unleashing,
  abbr={arXiv},
  title={Unleashing generalization of end-to-end autonomous driving with controllable long video generation},
  author={Ma, Erkang and Zhou, Lina and Tang, Tianyu and Zhang, Zhiyu and Han, Dong and Jiang, Jiangtao and Zhan, Kun and Jia, Ping and Lang, Xiaoxue and Yu, Kai},
  journal={arXiv preprint arXiv:2406.01349},
  year={2024},
  selected={true},
  preview={video_generation.png},
  abstract={This research introduces a novel approach for enhancing end-to-end autonomous driving systems through controllable long video generation. The method addresses the generalization challenges in autonomous driving by generating diverse, realistic driving scenarios for training and evaluation. By leveraging advanced video generation techniques with specific control mechanisms, the system can create extended driving sequences that maintain temporal consistency while representing a wide range of driving conditions and edge cases.},
  html={https://arxiv.org/abs/2406.01349}
}
